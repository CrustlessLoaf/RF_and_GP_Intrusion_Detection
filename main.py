# -*- coding: utf-8 -*-
"""classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yKXQIcn1wkOLewOTqzTZjU1izgU8sj_D

# Initial Import
"""

!pip install gplearn

import os
import pandas as pd
import numpy as np
import sklearn.ensemble as sk_ensemble
import sklearn.model_selection as sk_model_select
from sklearn.metrics import accuracy_score
from google.colab import drive
import gplearn as g
drive.mount('/content/drive')

"""# Random Forest Practice

"""

# cleaned_all_features = pd.read_csv('/content/drive/MyDrive/Le_Data/CICIDS2017_Cleaned_All_Features.csv')
filtered_balanced = pd.read_csv('/content/drive/MyDrive/Le_Data/CICIDS2017_RFE_binary_multiclass_balanced.csv')
# filtered_unbalanced = pd.read_csv('/content/drive/MyDrive/Le_Data/CICIDS2017_RFE_binary_multiclass_imbalanced.csv')
# filtered_balanced['Label']

# cleaned_all_features_labels = cleaned_all_features.pop('Label')
# cleaned_all_features_binary = cleaned_all_features.pop('BENIGN')
# 
filtered_balanced_labels = filtered_balanced.pop('Label')
filtered_balanced_binary = filtered_balanced.pop('BENIGN')

# filtered_unbalanced_labels = filtered_unbalanced.pop('Label')
# filtered_unbalanced_binary = filtered_unbalanced.pop('BENIGN')

### HYPERPARAMS AND FOREST
n_estimators = 100
max_depth = 45
random_forest = sk_ensemble.RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, verbose=1)

x_train, x_test, y_train, y_test = sk_model_select.train_test_split(filtered_balanced, filtered_balanced_labels, test_size=0.2, random_state=42)
### Fit, predict, check accuracy
### BALANCED
random_forest.fit(x_train, y_train)
prediction_score = random_forest.score(x_test, y_test)
prediction_score

# Multiclass test 25 depth : 0.9983474478195161
# Multiclass test 40 depth : 0.9985618881642545
# Multiclass test 50 depth : 0.9985533786267649

x_train, x_test, y_train, y_test = sk_model_select.train_test_split(filtered_unbalanced, filtered_unbalanced_binary, test_size=0.2, random_state=42)

### UNBALANCED
random_forest.fit(x_train, y_train)
prediction_score = random_forest.score(x_test, y_test)
prediction_score

# Test accuracy 10 depth, 0.9969641568949178
# Test accuracy 25 depth, 0.9989709605782423

x_train, x_test, y_train, y_test = sk_model_select.train_test_split(cleaned_all_features, cleaned_all_features_labels, test_size=0.2, random_state=42)

### ALL FEATURES
random_forest.fit(x_train, y_train)
prediction_score = random_forest.score(x_test, y_test)
prediction_score

# Test accuracy, 0.9989409027257168
# Multiclass test : 0.9986650777260704

from sklearn.metrics import confusion_matrix, plot_confusion_matrix
confusion_matrix(y_test, random_forest.predict(x_test))

# Full conf matrix for cleaned all features (no hyperparams) tn=111020, fn=301, tp=453957, fp=298  => falsealarms = 0.0027      missed = 0.000662619
# Full conf matrix for unbalanced dataset   (maxdepth 10   ) tn=110188, fn=418, tp=453671, fp=1299 => falsealarms = 0.011651583 missed = 0.000920524
# Full conf matrix for unbalanced dataset   (maxdepth 25   ) tn=111211, fn=306, tp=453783, fp=276  => falsealarms = 0.002475625 missed = 0.000673877
# False alarm rate : fp / (fp + tn)
# Missed detections: fn / (fn + tp)

"""# GP Failed attempt

"""

from sklearn.utils import gen_even_slices
# Initialization of a Population of GP trees
from random import random, randint, seed
from statistics import mean
from copy import deepcopy

population_size  = 500     # population size, from books, must be at least 500
minimum_depth    = 3      # minimal initial random tree depth
maximum_depth    = 10     # maximal initial random tree depth
gen_its          = 250    # maximal number of generations to run evolution, also known as run size, 10-50
tournament_size  = 5      # size of tournament for tournament selection
xo_rate          = 0.8   # crossover rate 
mutation_prob    = 0.25   # per-node mutation probability 
num_classes = 10

from sympy import symbols

def add(x, y): return int(x + y)
def sub(x, y): return int(x - y)
def mul(x, y): return int(x * y)
def Pdiv(x, y):           # Protected Division Operator
  if x == 0 or y == 0:
    return 1
  else:
    return int(x/y)

function_set = [add, sub, mul, Pdiv]  # Branches
terminal_set = [] # Leaves
# Terminal set is the set of inputs to the GP which can be expresexternal inputs, constants, functions with no args, 
#? So maybe we have to add in external inputs such as class variable, x_train data, y_train data?
for i in range(0, 30):
  if i <= num_classes:
    terminal_set.append(symbols('c{}'.format(i)))  # Saw this somewhere, not sure about this X thing, but the other coefficients make sense
  else: 
    terminal_set.append(randint(0,10.0))  # This random range was defined in literature
terminal_set[0:15]

def init_pop(number_bits, size):
    
    #taking samples over the distributtion
    #such that no. of 0 almost= no. of 1
    
    tot = int(2**number_bits)
    u = tot // size
    l = 1
    pop = []
    for i in range(size):
        p =[]
        num = randint(l,u)
        p = conver_bin(num)
        p = ([0]*(number_bits-len(p))) + p
        l = u+1
        u = (i+2) * (tot//size)
        pop.append(p)
    return pop

def battle(p1, p2)  # the battle funciton 
  if p1 > p2: # Checks if p1's classification score is higher than p2
    return p1
  else:
    return p2

def selection(tournament_size, gen):  # We will battle 5 contestants and one will be the best for breeding with the next winner
  p = {}
  for i in range(0, tournament_size):
    temp = gen[randint(0, population_size)]   # Grab a generation member at random for battle 
    while temp in p:    # While the same member was picked in p already, pick another and replace temp member
      temp = gen[randint(0, population_size)]
    p[i] = temp  # For index i put member there
  
  # Now we have our 5 battling participants form 0->4 indexed
  # Battles Begins [FFVII] theme commences
  # have p0 battle p1, p2 battle p3, and the two winners battle eachother and the final winner battles p4
  winner1 = battle(p0, p1)
  winner2 = battle(p2, p3)
  semi_final = battle(winner1, winner2)
  final = battle(semi_final, p4)
  return final

def sexy_time():  # crossover function
  parental_figure1 = selection(tournament_size, gen)
  parental_figure2 = selection(tournament_size, gen)
  while parental_figure1 == parental_figure2:
    parental_figure2 = selection(tournament_size, gen)
  # We need to exchange subtrees between the two after this statement
  # suggested from literature to choose only one of the offspring so maybe have a tournament between the two
  child1 =[]
  child2=[]
  winner_of_the_womb = battle(child1, child2)
  return winner_of_the_womb

"""Fitness functions determine the n-class abaility. If we can makje a fitness function which can classify different attacks then we can use this as the fitness metric."""

def fitness():  # The ability to identify the correct attacks in the dataset
  # How tf are we going to do this? IDK lol
  # We can probably input the test x and see if the y test is close to the predicted?

!pip install gplearn

from gplearn import genetic as g
import gplearn
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

import os
import pandas as pd
import numpy as np
import sklearn.ensemble as sk_ensemble
import sklearn.model_selection as sk_model_select
from sklearn.metrics import accuracy_score
from google.colab import drive
import gplearn as gp

"""#Using GPlearn Failed Attempt

"""

''' mapping = {
    'BENIGN': 0,
    'DDoS': 1,
    'PortScan': 2,
    'Bot': 3,
    'Infiltration': 4,
    'Brute Force': 5,
    'XSS': 6,
    'Sql Injection': 7,
    'FTP-Patator': 8,
    'SSH-Patator': 9,
    'DoS slowloris': 10,
    'DoS Slowhttptest': 11,
    'DoS Hulk': 12,
    'DoS GoldenEye': 13,
    'Heartbleed': 14
}

def encode_test(y):
  ### TRY THIS, COULD PROBABLY WORK
  y.replace(mapping)
  return y

def encode(y):
    for i in range(0, len(y)):
        if y[i] == 'BENIGN':
            y[i] = 0
        elif y[i] == 'DDoS':
            y[i] = 1
        elif y[i] == 'PortScan':
            y[i] = 2
        elif y[i] == 'Bot':
            y[i] = 3
        elif y[i] == 'Infiltration':
            y[i] = 4
        elif y[i] == 'Brute Force':
            y[i] = 5
        elif y[i] == 'XSS':
            y[i] = 6
        elif y[i] == 'Sql Injection':
            y[i] = 7
        elif y[i] == 'FTP-Patator':
            y[i] = 8
        elif y[i] == 'SSH-Patator':
            y[i] = 9
        elif y[i] == 'DoS slowloris':
            y[i] = 10
        elif y[i] == 'DoS Slowhttptest':
            y[i] = 11
        elif y[i] == 'DoS Hulk':
            y[i] = 12
        elif y[i] == 'DoS GoldenEye':
            y[i] = 13
        elif y[i] == 'Heartbleed':
            y[i] = 14
        else:
            exit('There was no class found')
    return y'''

net_data = pd.read_csv('/content/drive/MyDrive/Le_Data/CICIDS2017_RFE_binary_multiclass_balanced.csv')
#print(net_data.head())
y = net_data['Label']
print(y.head())
others = {}     # Stores the types of attacks
for classes in y:
    if classes in others.keys():
        others[classes] = others[classes] +1
    else:
        others[classes] = 1
print(others)
num_classes = len(others.keys())

classifier = g.SymbolicRegressor(population_size=7500,
                           generations=12, stopping_criteria=0.01,
                           p_crossover=0.65, p_subtree_mutation=0.2,
                           p_hoist_mutation=0.05, p_point_mutation=0.09,
                           max_samples=0.9, verbose=1,
                           parsimony_coefficient=0.01, random_state=40)

y = pd.read_csv('/content/drive/MyDrive/Le_Data/CICIDS2017_multi_encoded_y.csv')

x = net_data.drop(axis= 0, columns= ['Label', 'BENIGN'])

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

print(y_train.head())
print(y_test.head())
y_train.drop(columns= 'Unnamed: 0', inplace=True)
y_test.drop(columns= 'Unnamed: 0', inplace=True)

y_train = np.transpose(np.array(y_train))
y_test = np.transpose(np.array(y_test))

y_train = y_train[0,:]
y_test = y_test[0,:]

classifier.fit(np.array(X_train), y_train)

y_gp = classifier.predict(np.array(X_test))

score_gp = classifier.score(X_test, y_test)

lemon2 = pd.read_csv('lemon2(2).csv')

print(score_gp)

y_anal = pd.DataFrame(y_test)

y_malicious_test = y_anal[y_anal[0]!=0]
y_malicious_test

lemon2.drop(columns= 'Unnamed: 0', inplace= True)

malicious_lemon = pd.DataFrame()

for i in y_malicious_test.index:
  malicious_lemon[i] = lemon2.iloc[i]

malicious_lemon = malicious_lemon.transpose()

class1 = pd.DataFrame()

class1 = y_malicious_test.loc[y_malicious_test[0] == 1]
class1[1] = lemon2
class1

class2 = y_malicious_test.loc[y_malicious_test[0] == 2]
class2[1] = lemon2
class2

class3 = y_malicious_test.loc[y_malicious_test[0] == 3]
class3[1] = lemon2
class3

class4 = y_malicious_test.loc[y_malicious_test[0] == 4]
class4[1] = lemon2
class4

class5 = y_malicious_test.loc[y_malicious_test[0] == 5]
class5[1] = lemon2
class5

class6 = y_malicious_test.loc[y_malicious_test[0] == 6]
class6[1] = lemon2
class6

class7 = y_malicious_test.loc[y_malicious_test[0] == 6]
class7[1] = lemon2
class7

from sklearn.cluster import KMeans
kmeans = KMeans(
init="random",
n_clusters=15,
n_init=50,
max_iter=100,
random_state=0
)

kmeans.fit(malicious_lemon)

wtf = pd.DataFrame(kmeans.labels_[:])

final = y_malicious_test
final[1] = wtf
final

final.to_csv('wtf.csv')





"""# Real GP"""

!pip install gplearn
import os
import pandas as pd
import numpy as np
import sklearn.ensemble as sk_ensemble
import sklearn.model_selection as sk_model_select
from sklearn.metrics import accuracy_score
import os
import pandas as pd
import numpy as np
import sklearn.ensemble as sk_ensemble
import sklearn.model_selection as sk_model_select
from sklearn.metrics import accuracy_score
from google.colab import drive
import gplearn as g
from gplearn import genetic as g
import gplearn
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from google.colab import drive
drive.mount('/content/drive')

net_data = pd.read_csv('/content/drive/MyDrive/Le_Data/CICIDS2017_BENIGN_redo.csv')

attacks = net_data.loc[net_data['Label'] != 'BENIGN']
attacks
benign = net_data.loc[net_data['Label'] == 'BENIGN']
benign
print("Size of Benign: {}, Size of Attacks: {}".format(len(benign), len(attacks)))

b_balanced = benign[0:120200]
b_balanced
attack_and_benign_balanced = pd.concat([attacks, b_balanced])
attack_and_benign_balanced

a_b_shuffle = attack_and_benign_balanced.sample(frac=1)
a_b_shuffle

x = a_b_shuffle.drop(axis= 0, columns= ['BENIGN', 'Unnamed: 0'])
x

y = a_b_shuffle['BENIGN']
y

x_train, y_train = x[0:int(len(x)*.6)], y[0:int(len(y)*.6)]
x_test, y_test = x[int(len(x)*.6):], y[int(len(y)*.6):]

others_train = {}
for classes in x_train['Label']:
    if classes in others_train.keys():
        others_train[classes] = others_train[classes] +1
    else:
        others_train[classes] = 1
others_train

others_test = {}
for classes in x_test['Label']:
    if classes in others_test.keys():
        others_test[classes] = others_test[classes] +1
    else:
        others_test[classes] = 1
others_test

x_train.drop(columns= 'Label', inplace= True)
x_train.columns

x_test.drop(columns= 'Label', inplace= True)
x_test.columns

print('There are {} features in the test set'.format(x_test.iloc[1].count()))
print('There are {} features in the train set'.format(x_train.iloc[1].count()))

bin = g.SymbolicClassifier(population_size=5000, generations=20, tournament_size=20,
                           stopping_criteria=0.0, const_range=(-1, 1), init_depth=(3, 8), parsimony_coefficient=0.003,
                           init_method='half and half', function_set=('add', 'sub', 'mul', 'div', 'sqrt', 'log', 'abs', 'neg', 'inv', 'max', 'min', 'sin', 'cos', 'tan'),
                           transformer='sigmoid', metric='log loss', p_crossover=0.9,
                           p_subtree_mutation=0.02, p_hoist_mutation=0.01,
                           p_point_mutation=0.01, p_point_replace=0.05, max_samples=1.0, 
                           feature_names=x_train.columns, warm_start=False, 
                           low_memory=False, n_jobs=1, verbose=1, random_state=0)

import keras
import tensorflow

feats = x_train.columns
feats

y_train = np.array(y_train)
y_train

bin.fit(x_train, y_train)

from sklearn.metrics import roc_auc_score, confusion_matrix, plot_confusion_matrix
pred = bin.predict(x_test)
roc_gp = roc_auc_score(pred, y_test)
print("The ROC is: {}".format(roc_gp))
resulter = confusion_matrix(y_test, pred)
print("The Confusion matrix is: {}\n\t\t\t\t\t{}".format(resulter[0], resulter[1]))
plot_confusion_matrix(bin, x_test, y_test)

import pickle
Pkl_Filename = "bin_gp.pkl"  

with open(Pkl_Filename, 'wb') as file:  
    pickle.dump(bin, file)

predictiony = pd.DataFrame(pred)

predictiony.to_csv('predictions_binary.csv')

bin.score(x_test, y_test)

TN, FP = resulter[0,0], resulter[0,1]
FN, TP = resulter[1,0], resulter[1,1]

accuracy = (TP + TN)/(TP + TN + FP + FN)
accuracy

precision = TP/(TP+FP)
precision

recall =  TP/(TP+FN)
recall

f1 =  (2*precision*recall)/(precision + recall)
f1

a_b_shuffle.to_csv('pre_split.csv')

x_test.to_csv('x_test.csv')
y_test.to_csv('y_test.csv')

x.to_csv('x.csv')
y.to_csv('y.csv')

resulter.sum() - len(y_test)

len(x_test)- len(predictiony)

len(a_b_shuffle)

!cat /proc/cpuinfo  #To see CPU specs,

"""# Real Forest"""

!pip install gplearn
import pickle
from google.colab import drive
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, roc_auc_score, accuracy_score
import sklearn.ensemble as sk_ensemble

drive.mount('/content/drive')

### CODE DONE LOCALLY FOR CREATING RF MODEL
import pickle
import pandas as pd
import matplotlib.pyplot as plt
import sklearn.ensemble as sk_ensemble
import sklearn.model_selection as sk_model_select
from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, roc_auc_score, accuracy_score, precision_score, recall_score, f1_score



dataset = pd.read_csv('CICIDS2017_RFE_binary_multiclass_balanced.csv')
multiclass = dataset['Label']
binary = dataset['BENIGN']
dataset = dataset.drop(axis=0, columns=['Label', 'BENIGN'])

x_train, x_test, y_train, y_test = sk_model_select.train_test_split(dataset, multiclass, test_size=0.6, random_state=42)

def random_forest():
    n_estimators = 100
    max_depth = 80
    random_forest = sk_ensemble.RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, verbose=1)

    random_forest.fit(x_train, y_train)
    with open('rf_multi.pkl', 'wb') as f:
        pickle.dump(random_forest, f)
    return random_forest

def full_rf():
    rf_model = random_forest()

    predictions = rf_model.predict(x_test)
    matrix = confusion_matrix(y_test, predictions)
    with open('multi_matrix.txt', 'w') as m:
        m.write(str(matrix))

    print("Accuracy:", accuracy_score(y_test, predictions))
    disp = ConfusionMatrixDisplay(matrix)
    disp.plot()
    plt.show()

full_rf()

# RF best performer
with open('rf_multi.pkl', 'rb') as f:
  rf : sk_ensemble.RandomForestClassifier = pickle.load(f)

rf_predictions = rf.predict(x_test)

multi_labels = [
    'BENIGN',
    'DoS GoldenEye',
    'Bot',
    'Heartbleed',
    'DoS Hulk',
    'FTP-Patator',
    'PortScan',
    'DDoS',
    'Brute Force',
    'XSS',
    'DoS Slowhttptest',
    'Infiltration',
    'SSH-Patator',
    'DoS slowloris',
    'Sql Injection'
]

precision_scores = precision_score(y_test, rf_predictions, labels=multi_labels, average=None)
recall_scores = recall_score(y_test, rf_predictions, labels=multi_labels, average=None)
f1_scores = f1_score(y_test, rf_predictions, labels=multi_labels, average=None)
accuracy = accuracy_score(y_test, rf_predictions)

print(precision_scores)
print(recall_scores)
print(f1_scores)
print(accuracy)

metrics = {label: {} for label in multi_labels}

for i in range(len(multi_labels)):
    metrics[multi_labels[i]]['precision'] = precision_scores[i]
    metrics[multi_labels[i]]['recall'] = recall_scores[i]
    metrics[multi_labels[i]]['f1-score'] = f1_scores[i]
    tabs = '\t'
    if multi_labels[i] == 'Bot' or multi_labels[i] == 'DDoS' or multi_labels[i] == 'XSS':
        tabs += '\t'
    if multi_labels[i] != 'DoS Slowhttptest' and multi_labels[i] != 'DoS GoldenEye' and multi_labels[i] != 'DoS slowloris' and multi_labels[i] != 'Sql Injection':
        tabs += '\t'
    print(multi_labels[i], ':', tabs, metrics[multi_labels[i]])

print(metrics)

### Get only stuff predicted as attack
data = pd.read_csv('/content/drive/MyDrive/Le_Data/round9/x.csv')
y = data['Label']

# Drop nonsense
x = data.drop(axis=0, columns=['Label', 'Unnamed: 0'])

# Exact same split
x_train, y_train = x[0:int(len(x)*.6)], y[0:int(len(y)*.6)]
x_test, y_test = x[int(len(x)*.6):], y[int(len(y)*.6):]

# RF best performer
with open('/content/drive/MyDrive/Le_Data/rf_multi.pkl', 'rb') as f:
  random_forest = pickle.load(f)

# GP best performer
with open('/content/drive/MyDrive/Le_Data/round9/bin_gp.pkl', 'rb') as f:
  gp_model = pickle.load(f)

p = gp_model.predict(x_test)
### Append p, x_test and y_test together
x_test['prediction'] = p
x_test['label'] = y_test

### Drop all rows where row for p == 0 (benign)
x_test = x_test.drop(x_test[x_test['prediction'] == 0].index)

### Reset y_test to y_test column in combined df and drop columns
y_test = x_test['label']
x_test = x_test.drop(axis=0, columns=['label', 'prediction'])

multi_labels = [
        'BENIGN',
        'DoS GoldenEye',
        'Bot',
        'Heartbleed',
        'DoS Hulk',
        'FTP-Patator',
        'PortScan',
        'DDoS',
        'Brute Force',
        'XSS',
        'DoS Slowhttptest',
        'Infiltration',
        'SSH-Patator',
        'DoS slowloris',
        'Sql Injection'
    ]
### Run prediction
rf_predictions = random_forest.predict(x_test)
matrix = confusion_matrix(
    y_test, 
    rf_predictions,
    labels=multi_labels
)

with open('/content/drive/MyDrive/Le_Data/multi_matrix.txt', 'w') as m:
    m.write(str(matrix))

print("Accuracy:", accuracy_score(y_test, rf_predictions))
disp = ConfusionMatrixDisplay(matrix)
disp.plot()
plt.show()

from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score

precision_scores = precision_score(y_test, rf_predictions, labels=multi_labels, average=None)
recall_scores = recall_score(y_test, rf_predictions, labels=multi_labels, average=None)
f1_scores = f1_score(y_test, rf_predictions, labels=multi_labels, average=None)
accuracy = accuracy_score(y_test, rf_predictions)

metrics = {label: {} for label in multi_labels}

for i in range(len(multi_labels)):
  metrics[multi_labels[i]]['precision'] = precision_scores[i]
  metrics[multi_labels[i]]['recall'] = recall_scores[i]
  metrics[multi_labels[i]]['f1-score'] = f1_scores[i]

precision_avg = 0
recall_avg = 0
f1_scores_avg = 0
num_labels = len(multi_labels)

for metric in metrics:
  print(metric, ':', metrics[metric])
  precision_avg += metrics[metric]['precision']
  recall_avg += metrics[metric]['recall']
  f1_scores_avg += metrics[metric]['f1-score']

precision_avg /= num_labels
recall_avg /= num_labels
f1_scores_avg /= num_labels

print('accuracy:', accuracy)
print('precision_avg:', precision_avg)
print('recall_avg:', recall_avg)
print('f1 avg:', f1_scores_avg)
